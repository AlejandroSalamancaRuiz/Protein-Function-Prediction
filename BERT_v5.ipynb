{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T14:54:31.140092Z",
     "iopub.status.busy": "2023-08-06T14:54:31.138722Z",
     "iopub.status.idle": "2023-08-06T14:54:39.887894Z",
     "shell.execute_reply": "2023-08-06T14:54:39.886891Z",
     "shell.execute_reply.started": "2023-08-06T14:54:31.140052Z"
    },
    "papermill": {
     "duration": 9.85331,
     "end_time": "2023-05-09T08:30:19.002985",
     "exception": false,
     "start_time": "2023-05-09T08:30:09.149675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gc\n",
    "from keras import backend as K\n",
    "from keras.layers import BatchNormalization, LayerNormalization, Dense, Embedding, Flatten, Dropout, Reshape, Layer, MultiHeadAttention\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T14:54:39.889959Z",
     "iopub.status.busy": "2023-08-06T14:54:39.889461Z",
     "iopub.status.idle": "2023-08-06T14:54:39.902062Z",
     "shell.execute_reply": "2023-08-06T14:54:39.900893Z",
     "shell.execute_reply.started": "2023-08-06T14:54:39.889935Z"
    },
    "papermill": {
     "duration": 0.018272,
     "end_time": "2023-05-09T08:30:19.030432",
     "exception": false,
     "start_time": "2023-05-09T08:30:19.01216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"TensorFlow v\" + tf.__version__)\n",
    "print(\"Numpy v\" + np.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Predefined dictionary\n",
    "amino_dict = {'0':0, 'A': 1, 'R': 2, 'N': 3, 'D': 4, 'C': 5, 'Q': 6, 'E': 7, 'G': 8, \n",
    "              'H': 9, 'I': 10, 'L': 11, 'K': 12, 'M': 13, 'F': 14, 'P': 15, \n",
    "              'S': 16, 'T': 17, 'W': 18, 'Y': 19, 'V': 20, 'U':21, 'X':22, 'Z':23, 'O':24, 'B':25, 'Unk':26, '[CLF]':27}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sequence(fasta_file, amino_dict, seq_length):\n",
    "    \"\"\"\n",
    "    Load sequences from a FASTA file, convert them to numerical representations, and compute term frequency embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    fasta_file (str): Path to the FASTA file containing protein sequences.\n",
    "    amino_dict (dict): Dictionary mapping amino acids to numerical values.\n",
    "    seq_length (int): Length to which sequences should be truncated or padded.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - vectorized_data (np.ndarray): Array of truncated/padded sequences.\n",
    "        - ids (np.ndarray): Array of sequence IDs.\n",
    "        - org_sequences (list): List of original sequences.\n",
    "        - lengths (np.ndarray): Array of original sequence lengths.\n",
    "        - tf_embeddings (np.ndarray): Array of term frequency embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('File to process:', fasta_file)\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    list_ids, org_sequences, trunc_sequences, lengths, c, tf_embeddings = [], [], [], [], 0, []\n",
    "    \n",
    "    # Parse the FASTA file\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        \n",
    "        list_ids.append(record.id)  # Store sequence ID\n",
    "        \n",
    "        seq = np.array(list(record.seq))  # Convert sequence to numpy array\n",
    "\n",
    "        selected_tokens = np.zeros(seq_length)  # Initialize array for truncated/padded sequence\n",
    "\n",
    "        tf_rep = [1] * len(amino_dict.keys())  # Initialize term frequency representation\n",
    "\n",
    "        if seq.shape[0] >= seq_length:\n",
    "            # If sequence is longer than or equal to the desired length, truncate it\n",
    "            for i in range(seq_length):\n",
    "                tf_rep[amino_dict[seq[i]]] += 1  # Update term frequency\n",
    "                if seq[i] in amino_dict.keys():\n",
    "                    selected_tokens[i] = amino_dict.get(seq[i])  # Convert amino acid to numerical value\n",
    "                else:\n",
    "                    selected_tokens[i] = 25  # Use id 25 for unknown amino acids\n",
    "        else:\n",
    "            # If sequence is shorter than the desired length, pad it\n",
    "            for i in range(len(seq)):\n",
    "                tf_rep[amino_dict[seq[i]]] += 1  # Update term frequency\n",
    "                if seq[i] in amino_dict.keys():\n",
    "                    selected_tokens[i] = amino_dict.get(seq[i])  # Convert amino acid to numerical value\n",
    "                else:\n",
    "                    selected_tokens[i] = 25  # Use id 25 for unknown amino acids\n",
    "\n",
    "        tf_embeddings.append(tf_rep)  # Store term frequency representation\n",
    "        trunc_sequences.append(selected_tokens)  # Store truncated/padded sequence\n",
    "        org_sequences.append(seq)  # Store original sequence\n",
    "        lengths.append(len(record.seq))  # Store original sequence length\n",
    "        \n",
    "        c += 1\n",
    "        print(f'{c:.0f} proteins processed', end='\\r')  # Print progress\n",
    "    print()\n",
    "        \n",
    "    # Convert lists to numpy arrays\n",
    "    vectorized_data = np.array(trunc_sequences)\n",
    "    tf_embeddings = np.log(np.array(tf_embeddings))\n",
    "    ids = np.array(list_ids)\n",
    "    lengths = np.array(lengths)\n",
    "\n",
    "    # Sort data by sequence IDs\n",
    "    sorting_indexes = np.argsort(ids)\n",
    "    vectorized_data = vectorized_data[sorting_indexes]\n",
    "    tf_embeddings = tf_embeddings[sorting_indexes]\n",
    "    lengths = lengths[sorting_indexes]\n",
    "    ids = ids[sorting_indexes]\n",
    "\n",
    "    return vectorized_data, ids, org_sequences, lengths, tf_embeddings\n",
    "\n",
    "def load_labels(tsv_term_file, score_text_file, list_ids, num_labels_f, min_score):\n",
    "    \"\"\"\n",
    "    Load labels for protein sequences based on term frequency and score.\n",
    "\n",
    "    Parameters:\n",
    "    tsv_term_file (str): Path to the TSV file containing terms.\n",
    "    score_text_file (str): Path to the text file containing scores.\n",
    "    list_ids (list): List of protein sequence IDs.\n",
    "    num_labels_f (int): Number of most frequent terms to consider.\n",
    "    min_score (float): Minimum score threshold for terms.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - label_matrix (np.ndarray): Matrix of labels for protein sequences.\n",
    "        - label_list (list): List of terms that meet the criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    series_train_protein_ids = pd.Series(list_ids)  # Convert list of IDs to a pandas Series\n",
    "    \n",
    "    train_terms = pd.read_csv(tsv_term_file, sep=\"\\t\")  # Load terms from TSV file\n",
    "\n",
    "    label_list_f = train_terms['term'].value_counts().index[:num_labels_f].tolist()  # Get most frequent terms\n",
    "\n",
    "    score_data = np.genfromtxt(score_text_file, dtype=None, encoding=None)  # Load scores from text file\n",
    "    scores = np.zeros(score_data.shape[0])  # Initialize array for scores\n",
    "\n",
    "    for i in range(score_data.shape[0]):\n",
    "        scores[i] = score_data[i][1]  # Populate scores array\n",
    "    \n",
    "    indexes = np.where(scores > min_score)[0]  # Get indexes of scores above the minimum score\n",
    "\n",
    "    label_list_s = score_data[indexes]  # Get terms with scores above the minimum score\n",
    "\n",
    "    label_list = []  # Initialize list for final terms\n",
    "\n",
    "    for j in range(len(label_list_f)):\n",
    "        for k in range(label_list_s.shape[0]):\n",
    "            if label_list_f[j] == label_list_s[k][0]:\n",
    "                label_list.append(label_list_f[j])  # Add term to final list if it meets the criteria\n",
    "\n",
    "    num_labels = len(label_list)  # Get the number of final terms\n",
    "\n",
    "    print(f'The number of terms with a minimum score of {min_score} that are also in the most {num_labels_f} frequent terms is {num_labels}')\n",
    "\n",
    "    label_matrix = np.zeros((len(list_ids), num_labels))  # Initialize label matrix\n",
    "    \n",
    "    for i in range(num_labels):\n",
    "        \n",
    "        n_train_terms = train_terms[train_terms['term'] == label_list[i]]  # Get terms matching the final list\n",
    "        \n",
    "        label_related_proteins = n_train_terms['EntryID'].unique()  # Get unique protein IDs for the term\n",
    "\n",
    "        label_matrix[:,i] =  series_train_protein_ids.isin(label_related_proteins).astype(float)  # Populate label matrix\n",
    "        \n",
    "        p = i/num_labels * 100  # Calculate progress percentage\n",
    "        \n",
    "        print(f'{p:.0f}% completed', end='\\r')  # Print progress\n",
    "    \n",
    "    return label_matrix, label_list  # Return label matrix and final list of terms\n",
    "\n",
    "def load_protein_embd(embd_file, ids_file, del_duplicate=False):\n",
    "    \"\"\"\n",
    "    Load protein embeddings and their corresponding IDs from specified files.\n",
    "\n",
    "    Args:\n",
    "        embd_file (str): Path to the file containing the protein embeddings.\n",
    "        ids_file (str): Path to the file containing the protein IDs.\n",
    "        del_duplicate (bool, optional): If True, deletes the duplicate entry at index 632. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - ids (numpy.ndarray): Sorted array of protein IDs.\n",
    "            - embeddings (numpy.ndarray): Sorted array of protein embeddings corresponding to the IDs.\n",
    "    \"\"\"\n",
    "    embeddings = np.load(embd_file)\n",
    "    ids = np.load(ids_file)\n",
    "    sorting_indexes = np.argsort(ids)\n",
    "    embeddings = embeddings[sorting_indexes]\n",
    "    ids = ids[sorting_indexes]\n",
    "\n",
    "    if del_duplicate:\n",
    "        embeddings = np.delete(embeddings, 632, 0)\n",
    "        ids = np.delete(ids, 632, 0)\n",
    "\n",
    "    return ids, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_AMINO_SEQ = 300\n",
    "NUM_FREQ_TERMS = 550\n",
    "THRESHOLD_VAL = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training sequences and their term frequency embeddings\n",
    "train_seq_tokens, train_ids, train_org_seq, train_org_seq_lengths, tf_embd_train = load_sequence(\n",
    "    \"Train/train_sequences.fasta\", amino_dict, LEN_AMINO_SEQ)\n",
    "\n",
    "# Load test sequences and their term frequency embeddings\n",
    "test_seq_tokens, test_ids, test_org_seq, test_org_seq_lengths, tf_embd_test = load_sequence(\n",
    "    \"Test (Targets)/testsuperset.fasta\", amino_dict, LEN_AMINO_SEQ)\n",
    "\n",
    "# Remove the weird entry in the test set\n",
    "test_seq_tokens = np.delete(test_seq_tokens, 632, 0)\n",
    "tf_embd_test = np.delete(tf_embd_test, 632, 0)\n",
    "test_org_seq_lengths = np.delete(test_org_seq_lengths, 632, 0)\n",
    "test_ids = np.delete(test_ids, 632, 0)\n",
    "test_org_seq.pop(632)\n",
    "\n",
    "# Load protein embeddings and their corresponding IDs for training and test sets\n",
    "ids_train_embeddings, train_embeddings = load_protein_embd('ems2/train_embeddings.npy', 'ems2/train_ids.npy')\n",
    "ids_test_embeddings, test_embeddings = load_protein_embd('ems2/test_embeddings.npy', 'ems2/test_ids.npy')\n",
    "\n",
    "# Load labels for the training set\n",
    "train_labels, label_list = load_labels(\"Train/train_terms.tsv\", 'IA.txt', train_ids, NUM_FREQ_TERMS, THRESHOLD_VAL)\n",
    "\n",
    "# Create classification tokens for training and test sets\n",
    "clf_tokens_train = np.ones((tf_embd_train.shape[0], 1)) * 27\n",
    "clf_tokens_test = np.ones((tf_embd_test.shape[0], 1)) * 27\n",
    "\n",
    "# Combine embeddings, term frequency embeddings, sequence tokens, and classification tokens into a single matrix for training and test sets\n",
    "train_matrix = np.hstack((np.hstack((np.hstack((train_embeddings, tf_embd_train)), train_seq_tokens)), clf_tokens_train))\n",
    "test_matrix = np.hstack((np.hstack((np.hstack((test_embeddings, tf_embd_test)), test_seq_tokens)), clf_tokens_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer BERT-type Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardMLP(Layer):\n",
    "    \"\"\"\n",
    "    A simple feed-forward multi-layer perceptron (MLP) with two dense layers and dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the FeedForwardMLP layer.\n",
    "\n",
    "        Args:\n",
    "            n_embd (int): The embedding dimension.\n",
    "            dropout (float): Dropout rate.\n",
    "            **kwargs: Additional keyword arguments for the Layer base class.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.net = tf.keras.Sequential([\n",
    "            Dense(4 * n_embd, activation='relu'),  # First dense layer with ReLU activation\n",
    "            Dense(n_embd),  # Second dense layer\n",
    "            Dropout(dropout)  # Dropout layer\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the FeedForwardMLP.\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after passing through the MLP.\n",
    "        \"\"\"\n",
    "        return self.net(inputs)\n",
    "\n",
    "\n",
    "class Block(Layer):\n",
    "    \"\"\"\n",
    "    A transformer block consisting of multi-head self-attention and feed-forward layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, dropout, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the Block layer.\n",
    "\n",
    "        Args:\n",
    "            n_embd (int): The embedding dimension.\n",
    "            n_head (int): Number of attention heads.\n",
    "            dropout (float): Dropout rate.\n",
    "            **kwargs: Additional keyword arguments for the Layer base class.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        head_size = n_embd // n_head\n",
    "\n",
    "        self.sa = MultiHeadAttention(n_head, key_dim=n_embd)  # Multi-head self-attention layer\n",
    "        self.fw = FeedForwardMLP(n_embd, dropout)  # Feed-forward MLP layer\n",
    "\n",
    "        self.ln1 = LayerNormalization()  # Layer normalization before self-attention\n",
    "        self.ln2 = LayerNormalization()  # Layer normalization before feed-forward MLP\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Block.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after passing through the block.\n",
    "        \"\"\"\n",
    "        q = self.ln1(x)\n",
    "        k, v = q, q\n",
    "\n",
    "        x = x + self.sa(q, k, v)  # Add and normalize after self-attention\n",
    "        x = x + self.fw(self.ln2(x))  # Add and normalize after feed-forward MLP\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Network(keras.Model):\n",
    "    \"\"\"\n",
    "    A custom neural network model consisting of embedding layers, transformer blocks, and a final output layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seq_length, n_heads, dict_length, embd_dim, num_blocks, dropout, out_dim, prot_embd_dim, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the Network model.\n",
    "\n",
    "        Args:\n",
    "            seq_length (int): Sequence length.\n",
    "            n_heads (int): Number of attention heads.\n",
    "            dict_length (int): Length of the dictionary (vocabulary size).\n",
    "            embd_dim (int): Embedding dimension.\n",
    "            num_blocks (int): Number of transformer blocks.\n",
    "            dropout (float): Dropout rate.\n",
    "            out_dim (int): Output dimension.\n",
    "            prot_embd_dim (int): Protein embedding dimension.\n",
    "            **kwargs: Additional keyword arguments for the Model base class.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.seq_length = seq_length\n",
    "        self.dict_length = dict_length\n",
    "        self.embd = prot_embd_dim\n",
    "\n",
    "        self.protein_embd = Dense(units=embd_dim, activation='relu')  # Protein embedding layer\n",
    "        self.tf_embd = Dense(units=embd_dim, activation='relu')  # TF embedding layer\n",
    "\n",
    "        self.token_embd = Embedding(input_dim=dict_length, output_dim=embd_dim, input_length=seq_length + 1)  # Token embedding layer\n",
    "        self.pos_embd = Embedding(input_dim=seq_length + 3, output_dim=embd_dim, input_length=seq_length + 3)  # Positional embedding layer\n",
    "        self.segment_embd = Embedding(input_dim=4, output_dim=embd_dim, input_length=seq_length + 5)  # Segment embedding layer\n",
    "\n",
    "        self.blocks = [Block(embd_dim, n_heads, dropout) for i in range(num_blocks)]  # List of transformer blocks\n",
    "\n",
    "        self.out = Dense(out_dim, \"sigmoid\")  # Output layer\n",
    "\n",
    "    def call(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the Network model.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after passing through the network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Split the input tensor into protein tokens, TF tokens, and sequence tokens\n",
    "        cut_1 = self.embd\n",
    "        cut_2 = cut_1 + self.dict_length\n",
    "    \n",
    "        # X_1 = Protein tokens, X_2 = TF tokens, X_seq = Sequence tokens\n",
    "        X_1, X_2, X_seq = X[:, 0:cut_1], X[:, cut_1:cut_2], X[:, cut_2:]\n",
    "        # Cast X_seq to integer type\n",
    "        X_seq = tf.cast(X_seq, tf.int32)\n",
    "        # Create a list of positional indices from 0 to seq_length + 3 (3 extra tokens: protein, TF, and CLF)\n",
    "        pos_list = np.arange(self.seq_length + 3)\n",
    "\n",
    "        # Create a list of segment indices (0 for CLF, 1 for protein, 2 for TF, 3 for sequence)\n",
    "        segments = np.ones(self.seq_length + 3)\n",
    "        segments[-1] = 0\n",
    "        segments[0:cut_1] = 1\n",
    "        segments[cut_1:cut_2] = 2\n",
    "        segments[cut_2:-1] = 3\n",
    "\n",
    "        # Embed the segment indices\n",
    "        segment_embd = self.segment_embd(segments)\n",
    "\n",
    "        # Embed the protein tokens\n",
    "        prot_token = self.protein_embd(X_1)\n",
    "        prot_token = prot_1_token[:, tf.newaxis, :]\n",
    "\n",
    "        # Embed the TF tokens\n",
    "        tok_tf_embd = self.tf_embd(X_2)\n",
    "        tok_tf_embd = tok_tf_embd[:, tf.newaxis, :]\n",
    "\n",
    "        # Embed the sequence tokens\n",
    "        tok_seq_embd = self.token_embd(X_seq)\n",
    "        pos_embd = self.pos_embd(pos_list)\n",
    "\n",
    "        # Concatenate the embeddings\n",
    "        tokens = tf.concat([prot_token, tok_tf_embd, tok_seq_embd], axis=1)\n",
    "\n",
    "        # Add the positional and segment embeddings\n",
    "        Z = tokens + pos_embd + segment_embd\n",
    "\n",
    "        # Pass the embeddings through the transformer blocks\n",
    "        for block in self.blocks:\n",
    "            Z = block(Z)\n",
    "\n",
    "        # Extract the CLF token\n",
    "        CLF_token = Z[:, -1, :]\n",
    "        # Pass the CLF token through the output layer\n",
    "        CL = self.out(CLF_token)\n",
    "\n",
    "        return CL\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "sequence_length = train_seq_tokens.shape[1]\n",
    "protein_embedding_dim = train_embeddings.shape[1]\n",
    "output_dim = train_labels.shape[1]\n",
    "num_encoder_blocks = 10\n",
    "num_att_heads = 4\n",
    "dropout_rate = 0.2\n",
    "hidden_embedding_dim = 120\n",
    "\n",
    "model = Network(seq_length=sequence_length, \n",
    "                n_heads=num_att_heads, \n",
    "                dict_length=len(amino_dict.keys()), \n",
    "                embd_dim=prot, \n",
    "                num_blocks=num_encoder_blocks, \n",
    "                dropout=dropout_rate, \n",
    "                out_dim=output_dim, \n",
    "                prot_embd_dim=protein_embedding_dim)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy', tf.keras.metrics.AUC()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T16:25:15.149139Z",
     "iopub.status.busy": "2023-07-30T16:25:15.148792Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_matrix, train_labels, epochs=4, validation_split=0.1, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019782,
     "end_time": "2023-05-09T08:41:01.06997",
     "exception": false,
     "start_time": "2023-05-09T08:41:01.050188",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Plot the model's loss and accuracy for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.647806,
     "end_time": "2023-05-09T08:41:01.737745",
     "exception": false,
     "start_time": "2023-05-09T08:41:01.089939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss']].plot(title=\"Cross-entropy\")\n",
    "history_df.loc[:, ['binary_accuracy']].plot(title=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021665,
     "end_time": "2023-05-09T08:41:01.780867",
     "exception": false,
     "start_time": "2023-05-09T08:41:01.759202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_matrix\n",
    "del train_ids\n",
    "del tf_embd_train\n",
    "del ids_train_embeddings\n",
    "del train_embeddings\n",
    "del train_seq_tokens\n",
    "del train_org_seq\n",
    "del train_org_seq_lengths\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "#test_matrix = np.random.randint(low=0, high=len(amino_dict.keys()), size=(141865,1700))\n",
    "predictions =  model.predict(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T14:55:04.387932Z",
     "iopub.status.busy": "2023-08-06T14:55:04.387556Z",
     "iopub.status.idle": "2023-08-06T14:55:04.394574Z",
     "shell.execute_reply": "2023-08-06T14:55:04.393459Z",
     "shell.execute_reply.started": "2023-08-06T14:55:04.387904Z"
    }
   },
   "outputs": [],
   "source": [
    "def submission(predictions, test_ids, label_list):\n",
    "    \n",
    "    df_submission = pd.DataFrame(columns = ['Protein Id', 'GO Term Id','Prediction'])\n",
    "    l = []\n",
    "    for k in test_ids:\n",
    "        l += [ k] * predictions.shape[1]   \n",
    "\n",
    "    df_submission['Protein Id'] = l\n",
    "    df_submission['GO Term Id'] = label_list * predictions.shape[0]\n",
    "    df_submission['Prediction'] = predictions.ravel()\n",
    "    df_submission.to_csv(\"submission.tsv\",header=False, index=False, sep=\"\\t\")\n",
    "    \n",
    "    return df_submission\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-06T14:56:07.412956Z",
     "iopub.status.busy": "2023-08-06T14:56:07.412651Z",
     "iopub.status.idle": "2023-08-06T14:58:36.810392Z",
     "shell.execute_reply": "2023-08-06T14:58:36.809116Z",
     "shell.execute_reply.started": "2023-08-06T14:56:07.412931Z"
    }
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del test_matrix\n",
    "del tf_embd_test\n",
    "del ids_test_embeddings\n",
    "del test_embeddings\n",
    "del test_seq_tokens\n",
    "del test_org_seq\n",
    "del test_org_seq_lengths\n",
    "gc.collect()\n",
    "\n",
    "df_submission = submission(predictions, test_ids, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
